```md
# RAG Chatbot (Groq + Ollama + FAISS)

This project is a **Retrieval-Augmented Generation (RAG) chatbot** built using:
- **Groq API** for fast LLM inference
- **Ollama** for local embeddings
- **FAISS** as the vector database
- **Streamlit** for the frontend UI

---

## ğŸ“ Project Structure
```

RAG-CHATBOT/
â”‚
â”œâ”€â”€ frontend.py          # Streamlit UI
â”œâ”€â”€ rag_pipeline.py      # RAG logic (retrieval + LLM)
â”œâ”€â”€ vector_database.py   # PDF loading, chunking, FAISS
â”œâ”€â”€ pdfs/                # Uploaded PDFs
â”œâ”€â”€ .env                 # API keys (ignored by git)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

````

---

## âš™ï¸ Prerequisites
- Python **3.10+**
- Git
- Internet connection (for Groq API)
- Ollama installed locally

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/anushkaa-dubey/rag-chatbot.git
cd rag-chatbot
````

---

### 2ï¸âƒ£ Create and activate virtual environment (recommended)

```bash
python -m venv .venv
```

**Windows**

```bash
.venv\Scripts\activate
```

**Mac/Linux**

```bash
source .venv/bin/activate
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install streamlit langchain langchain-groq langchain-community langchain-ollama faiss-cpu python-dotenv pdfplumber
```

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the project root:

```env
GROQ_API_KEY="your_groq_api_key_here"
```

âš ï¸ `.env` is ignored by git for security reasons.

---

## ğŸ¤– Ollama Setup (Local Embeddings)

### Install Ollama

Download from ğŸ‘‰ [https://ollama.com](https://ollama.com)

### Start Ollama server

```bash
ollama serve
```

### Pull embedding model

```bash
ollama pull deepseek-r1:1.5b
```

---

## â–¶ï¸ Run the Application

Open a **new terminal** (with venv activated):

```bash
python -m streamlit run frontend.py
```

Then open in browser:

```
http://localhost:8501
```

---

## ğŸ§  How the RAG Pipeline Works

1. User uploads a PDF
2. PDF is split into chunks
3. Chunks are embedded **locally using Ollama**
4. Embeddings are stored in **FAISS**
5. User query retrieves relevant chunks
6. Context + query sent to **Groq LLM**
7. Answer displayed in Streamlit UI

---

## âš ï¸ Important Notes

* FAISS index is static unless rebuilt after a new PDF upload
* Groq is used **only for generation**, not embeddings
* Ollama must be running before querying
* API keys should never be hardcoded

---

## ğŸš€ Future Improvements

* Rebuild FAISS index on every PDF upload
* Multi-PDF support
* Source citations in answers
* Better prompt tuning

---

## ğŸ‘©â€ğŸ’» Author

**Anushkaa Dubey**

---

## ğŸ“œ License

This project is for educational and hackathon purposes only.

````

---

### âœ… Final step (donâ€™t forget)
```bash
git add README.md
git commit -m "Add README with setup instructions"
git push
````

If you want, next I can:

* Generate `requirements.txt`
* Clean RAG architecture (PDF â†’ FAISS â†’ Query)
* Make this **SIH / hackathon submission ready** ğŸ’ª
